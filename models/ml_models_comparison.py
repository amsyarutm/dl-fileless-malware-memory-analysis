# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# List files in the directory (optional, for verification)
import os
for dirname, _, filenames in os.walk('D:/Cybersecurity/KerasNLP'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Load the data
df = pd.read_csv('D:/Cybersecurity/KerasNLP/MalwareMemoryDump.csv')

# Basic data exploration
print("First five rows of the dataframe:")
print(df.head())

print("\nLast five rows of the dataframe:")
print(df.tail())

print("\nDataframe information:")
print(df.info())

print("\nDescriptive statistics:")
print(df.describe().T)  # Transposed for better readability

# Data profiling function
def data_profiling(df):
    data_profile = []
    columns = df.columns
    for col in columns:
        dtype = df[col].dtypes
        nunique = df[col].nunique()
        null = df[col].isnull().sum()
        duplicates = df[col].duplicated().sum()
        data_profile.append([col, dtype, nunique, null, duplicates])
    data_profile_finding = pd.DataFrame(data_profile)
    data_profile_finding.columns = ['column', 'dtype', 'nunique', 'null', 'duplicates']
    return data_profile_finding

# Print data profiling
print("Data Profiling:")
print(data_profiling(df))

# Null values count
print("\nNull values in each column:")
print(df.isnull().sum())

# Value counts for 'ClassTypeName' column
print("\nValue counts for 'ClassType':")
print(df['ClassType'].value_counts())

# Value counts for 'SubType' column
print("\nValue counts for 'SubType':")
print(df['SubType'].value_counts())  # Can add .head(n) to limit the output

# EDA
numeric_features = [i for i in df.columns if df[i].dtype != 'O']
categorical_features = [i for i in df.columns if df[i].dtype == 'O']

print(len(numeric_features))

# Correlation
# Excluding non-numeric columns for correlation
numerical_df = df.select_dtypes(include=[np.number])

# Plotting the heatmap
sns.set(rc={'figure.figsize': (45, 25)})
sns.heatmap(numerical_df.corr(), vmin=-1, vmax=1, cmap="PiYG", annot=True)
plt.show()

plt.figure(figsize=(25, 10))
sns.countplot(data=df, x='SubType', palette='Set2')
plt.show()

plt.figure(figsize=(25, 10))
sns.countplot(data=df, x='ClassType', palette='Set2')
plt.title("Distribution of Class Types")
plt.show()

# Feature Engineering

#### Removing the column which have no correlation with other data values
# - pslist_nprocs64bit
# - handles_nport
# - psxview_not_in_pslist_false_avg
# - svcscan_interactive_process_services 
# - callbacks_ngeneric
# - callbacks_nanonymous
# - Raw type
# - Sub Type

# Dropping specified columns
df = df.drop([
    'pslist_nprocs64bit',
    'handles_nport',
    'psxview_not_in_pslist_false_avg',
    'svcscan_interactive_process_services',
    'callbacks_ngeneric',
    'callbacks_nanonymous',
    'Raw_Type',
    'SubType'
], axis=1)

# Displaying first few rows to confirm the changes
print("First five rows after dropping columns:")
print(df.head())

# Splitting the dataframe into features (X) and target (y)
x = df.drop('ClassType', axis=1).values  # Features: All columns except 'ClassType'
y = df['ClassType'].values  # Target: 'ClassType' column

# Printing the shape of x
print("\nShape of feature matrix (x):", x.shape)

# Label Encoding the target variable
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)
print("Encoded target variable:", y)

# Train-Test Split & Feature Scaling
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

print(x_train.shape)
print(x_test.shape)

# Classification algorithms
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, \
    precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, auc

def classalgo_test(x_train, x_test, y_train, y_test):
    dc = DecisionTreeClassifier()
    rfc = RandomForestClassifier()
    gbc = GradientBoostingClassifier()
    xgb = XGBClassifier()
    bagging = BaggingClassifier()
    ada_boost = AdaBoostClassifier()
    
    algos = [dc, rfc, gbc, xgb, bagging, ada_boost]
    algo_names = ['DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 
                  'BaggingClassifier', 'XGBClassifier', 'AdaBoostClassifier']
    
    results = pd.DataFrame(index=algo_names, columns=['Train_Accuracy', 'Train_Precision', 'Train_F1Score', 'Train_Recall', 
                                                      'Test_Accuracy', 'Test_Precision', 'Test_F1Score', 'Test_Recall', 'Test_AUC'])
    
    for algo, name in zip(algos, algo_names):
        algo.fit(x_train, y_train)
        y_train_pred = algo.predict(x_train)
        y_test_pred = algo.predict(x_test)
        
        results.loc[name, 'Train_Accuracy'] = accuracy_score(y_train, y_train_pred)
        results.loc[name, 'Train_Precision'] = precision_score(y_train, y_train_pred, average='macro')
        results.loc[name, 'Train_F1Score'] = f1_score(y_train, y_train_pred, average='macro')
        results.loc[name, 'Train_Recall'] = recall_score(y_train, y_train_pred, average='macro')
        
        results.loc[name, 'Test_Accuracy'] = accuracy_score(y_test, y_test_pred)
        results.loc[name, 'Test_Precision'] = precision_score(y_test, y_test_pred, average='macro')
        results.loc[name, 'Test_F1Score'] = f1_score(y_test, y_test_pred, average='macro')
        results.loc[name, 'Test_Recall'] = recall_score(y_test, y_test_pred, average='macro')
        results.loc[name, 'Test_AUC'] = roc_auc_score(y_test, algo.predict_proba(x_test), multi_class='ovr')
        
    return results.sort_values('Test_Accuracy', ascending=False)

# Run the classifier tests
results = classalgo_test(x_train, x_test, y_train, y_test)
print(results)

# RandomForestClassifier details
rf = RandomForestClassifier()
rf.fit(x_train, y_train)
rf_predict = rf.predict(x_test)

# Confusion matrix
cm = confusion_matrix(y_test, rf_predict)
ConfusionMatrixDisplay(cm, display_labels=['Benign', 'Malware', 'Fileless']).plot()
plt.show()

# Classification report
print(classification_report(y_test, rf_predict, target_names=['Benign', 'Malware', 'Fileless']))

# ROC AUC for RandomForestClassifier
rf_roc_auc = roc_auc_score(y_test, rf.predict_proba(x_test), multi_class='ovr')
print(f"RandomForestClassifier ROC AUC: {rf_roc_auc}")

# ROC curve for RandomForestClassifier
fpr = {}
tpr = {}
roc_auc = {}

for i in range(3):
    fpr[i], tpr[i], _ = roc_curve(y_test, rf.predict_proba(x_test)[:, i], pos_label=i)
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure()
for i in range(3):
    plt.plot(fpr[i], tpr[i], lw=2, label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
